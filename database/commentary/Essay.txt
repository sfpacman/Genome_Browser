
1) Approach to the project
i) Interaction with the team
The team interacted quite well together, and met up at least one or more times a week to work on the project and discuss how it was going. Pak was very helpful in providing help when Chiara or I were stuck with debugging as he has more coding experience. Pak and I got together a couple days each week in the past three weeks before the project was due to work on the project together as it was easier for communication purposes. 
ii) Overall project requirements
Pak was looking to improve his skills with website design so he was assigned the front end, I was looking to continue to develop my coding skills for parsing and database creation so I was assigned the database and Chiara did not have a preference and was assigned the middle layer. Chiara and I both worked on the data access tier. We as a group discussed how the website would be laid out and how that would contribute to the database design.
ii) Requirements for my contribution
The requirements for my section were straightforward for the database creation as there was a similar assignment in the Data Management course, parsing the data was outlined in the requirement section, did more research on how the data access tier works and importing the data to fully understand what to do. 

2) Performance of the development cycle
The development cycle worked smoothly as we had a Trello group created outlining upcoming project goals by certain dates, a list of each of the group member’s requirements for their sections and things to worry about in the upcoming weeks, and the schema for the database so there was no confusion as to what things were named. We also had a GitHub group created to upload the coding that each of us had every step of the way so we all knew where we were at in our own tiers and to see any coding relevant to our own sections.

3) The development process
The design of the database didn’t take very long as we met and discussed what we were each planning and so the way the tables were created were to allow for an easier time accessing the database without needing to join too many tables together. I spent the most time on the parsing and figuring out the best strategy to go about it, attempting it and trying several different strategies before settling on one that worked the best. The parsing code initially written had a lot of issues and errors in it and took a very long time to debug and was a cause of a lot of frustration for many days. I also flip flopped between using Pandas to import the data or MySQL connectors, but settled on MySQL connectors. 

4) Code testing
The strategies for code testing, in no particular order, that I used were to proofread the code for silly errors or misspellings as a lot of my errors were that, when the code was working was to look at the data it was extracting to see if it was pulling out the correct information. Another strategy used was to scatter some print statements to see where the code was getting an error which I found to be very useful in spotting the misspellings. Since it wasn’t a very long code I did not use assertions to test.

5) Known issues
The known issues are that the table and column names are too long and have capitals creating some frustration on my end when I forgot to capitalize something further in the code. I intentionally made them longer as to be more explicit to the other group members so that nothing would be misinterpreted. The one error that I had that popped up while importing the data into the database was the DNA sequences and amino acid sequence were too big of an importing job that is wasn’t allowed nor did I have the authority to change the max allowed packet. This was circumvented by taking out one of the longest sequence genes and importing the rest of the genes into the database which worked but there is one gene missing.

6) What worked and what didn't - problems and solutions
The part of the database that took me the longest was figuring out how to parse the data using the Biopython GenBank module. I initially wrote a lot of regrexes to parse the data but then read more on each of the Biopythons modules. I also initially tried to use SeqIO but found that Genbank had more specific functions to obtain what I needed from the Chromosome 8 file. The file itself was missing a lot of random entry information in random genes so that created a new problem to solve after I looked at the data extracted as it was uneven in entry length. The exons also created a problem as some had multiple entries on one gene and some had none so a ‘for’ loop was created in when importing the data into the database. When importing that data there were errors that came out when switching between python and SQL so ASCII character coding had to be put in, in order to debug the code.
As a group I think that there were difficulties in matching schedules to meet up as there were different priorities at play as well as the consistency using Github. There was also a coding skill level difference in the group putting a little pressure on Pak to help problem solve initially until there was more coding being done to get the hang of problem solving more. 

7) Alternative strategies
The alternative solution was to use regrexes to parse the data as they are great for extracting very specific information from the file. I decided to use Biopython’s Genbank as it would be potentially very useful to know how to use in the future as I would like to work in disease research. Another alternative strategy mentioned earlier was using Pandas to import the data into the database. 

8) Personal insights
The personal insights I have had are a lot to do with debugging. Initially I would rewrite my whole code instead of just working out what the errors could be as I would think that what I had written just wasn’t doing what it was supposed to as a whole. It would sometimes take a whole day just to fix one mistake due to my inexperience with coding which was a source of huge frustration as it would take sometimes several days to just get one function to work in the parsing part and when using the SQL connector to upload the data. It was difficult to manage this huge amounts of time spent on debugging while trying to balance my time studying for the upcoming exams. I feel as though I learned how to debug a lot more efficiently and to test each section being written as I go instead of writing the whole code and then going back and trying to fix it. I still have a large learning curve with coding that will just come with more practise. 
I gained a much better understanding of how all the tiers fit in with each other as what was learned in class was sometimes in isolation so I wouldn’t fully understand the full associations of it so it gave me a better sense of what it was used for and why.  
